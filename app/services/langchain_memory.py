from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory
from langchain_google_genai import GoogleGenerativeAI
import google.generativeai as genai
import os
from dotenv import load_dotenv
from services import get_chat_messages
import vectorstore.faiss_utils as faiss_utils

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
env_path = os.path.join(os.path.dirname(__file__), "..", ".env")
load_dotenv(dotenv_path=env_path)

# âœ… Gemini API í‚¤ ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

genai.configure(api_key=GEMINI_API_KEY)

# âœ… LangChain Memory ì„¤ì • (ëŒ€í™” ê¸°ë¡ + ìš”ì•½ Memory + ê°œì²´ ì •ë³´ Memory)
buffer_memory = ConversationBufferMemory(memory_key="chat_history")  # ìµœê·¼ ëŒ€í™” ì €ì¥
summary_memory = ConversationSummaryMemory(
    llm=GoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21"), memory_key="summary"
)

def sync_memory_from_firestore(chat_id):
    """ğŸ”¥ Firestoreì—ì„œ ê°€ì ¸ì˜¨ ëŒ€í™” ê¸°ë¡ì„ LangChain Memoryì— ì¶”ê°€"""
    chat_history = get_chat_messages(chat_id)  # Firestoreì—ì„œ ê°€ì ¸ì˜¤ê¸°
    for message in chat_history:
        buffer_memory.save_context({"input": message["content"]}, {"output": ""})  # Memoryì— ì €ì¥
    print(f"âœ… Firestoreì—ì„œ {len(chat_history)}ê°œì˜ ë©”ì‹œì§€ë¥¼ ë¶ˆëŸ¬ì™€ Memoryì— ë™ê¸°í™” ì™„ë£Œ!")

def sync_memory_from_faiss(chat_id, user_input):
    """ğŸ”¥ FAISSì—ì„œ ê°€ì ¸ì˜¨ ìœ ì‚¬í•œ ë¬¸ì¥ì„ LangChain Memoryì— ì¶”ê°€"""
    similar_messages = faiss_utils.get_similar_messages(chat_id, user_input, top_k=3)
    if similar_messages:
        buffer_memory.save_context({"input": user_input}, {"output": similar_messages})
        print(f"âœ… FAISS ê²€ìƒ‰ëœ ë¬¸ì¥ {len(similar_messages)}ê°œë¥¼ Memoryì— ì¶”ê°€!")

def add_message_to_memory(user_message: str, ai_response: str):
    """ğŸ”¥ LangChain Memoryì— ì‚¬ìš©ìì™€ AIì˜ ëŒ€í™”ë¥¼ ì¶”ê°€í•˜ë©´ì„œ ìš”ì•½ ê°œì„ """
    buffer_memory.save_context({"input": user_message}, {"output": ai_response})

    # âœ… ê¸°ì¡´ ìš”ì•½ì„ ë¶ˆëŸ¬ì˜´
    current_summary = summary_memory.load_memory_variables({}).get("summary", "")

    # âœ… ìƒˆë¡œìš´ ì •ë³´ë¥¼ ìš”ì•½í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½
    updated_summary = f"{current_summary}\n[ì‚¬ìš©ì]: {user_message}\n[AI]: {ai_response}"

    # âœ… ì¤‘ë³µ ë¬¸ì¥ ì œê±° ë° í•µì‹¬ ì •ë³´ë§Œ ìœ ì§€
    updated_summary = clean_summary(updated_summary)

    summary_memory.save_context({"input": user_message}, {"output": updated_summary})
    
def generate_response(user_input: str, chat_id: str) -> str:
    """ğŸ”¥ LangChain Memory + FAISSë¥¼ í™œìš©í•œ AI ì‘ë‹µ ìƒì„±"""
    sync_memory_from_firestore(chat_id)  # Firestore ë°ì´í„° ë°˜ì˜
    sync_memory_from_faiss(chat_id, user_input)  # FAISS ê²€ìƒ‰ ê²°ê³¼ ë°˜ì˜

    conversation_summary = get_conversation_summary()
    full_input = f"{conversation_summary}\n\n{user_input}"

    response = genai.GenerativeModel("gemini-2.0-flash-thinking-exp-01-21").generate_content([full_input])

    if not response.text:
        return "ì£„ì†¡í•´ìš”, ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ì–´ìš”."

    ai_response = response.text.strip()
    
    # âœ… Memoryì— ëŒ€í™” ì¶”ê°€
    add_message_to_memory(user_input, ai_response)
    
    return ai_response

def get_conversation_summary():
    """ğŸ”¥ LangChain Memoryì—ì„œ ëŒ€í™” ìš”ì•½ì„ ê°€ì ¸ì˜¤ë©´ì„œ ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±°"""
    summary = summary_memory.load_memory_variables({}).get("summary", "")

    # âœ… í•µì‹¬ ë‹¨ì–´ ìœ„ì£¼ë¡œ ìš”ì•½ ì••ì¶•
    return refine_summary(summary)

def get_conversation_history():
    """LangChain Memoryì—ì„œ ì „ì²´ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°"""
    return buffer_memory.load_memory_variables({}).get("chat_history", "")

def clean_summary(summary):
    """ğŸ”¥ ì¤‘ë³µ ë¬¸ì¥ ì œê±° ë° í•µì‹¬ ì •ë³´ë§Œ ìœ ì§€í•˜ëŠ” í•¨ìˆ˜"""
    lines = summary.split("\n")
    unique_lines = []
    seen = set()

    for line in lines:
        if line not in seen:  # ì¤‘ë³µ ë°©ì§€
            seen.add(line)
            unique_lines.append(line)

    # âœ… ìµœì‹  500ìê¹Œì§€ë§Œ ìœ ì§€í•˜ì—¬ ìš”ì•½ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ëŠ” ê²ƒ ë°©ì§€
    cleaned_summary = "\n".join(unique_lines)
    return cleaned_summary[-500:] if len(cleaned_summary) > 500 else cleaned_summary

def refine_summary(summary):
    """ğŸ”¥ ì¤‘ìš”í•œ ì •ë³´ë§Œ ë‚¨ê¸°ê³  ë¶ˆí•„ìš”í•œ ë¬¸ì¥ ì œê±°"""
    keywords = ["ì¢‹ì•„í•˜ëŠ”", "ì·¨ë¯¸", "ê¸°ì–µ", "ë§ì”€", "ê³ì— ìˆì„ê²Œìš”"]  # ì¤‘ìš”í•œ í‚¤ì›Œë“œ
    lines = summary.split("\n")
    refined_lines = [line for line in lines if any(keyword in line for keyword in keywords)]

    refined_summary = "\n".join(refined_lines)
    return refined_summary[-500:] if len(refined_summary) > 500 else refined_summary